# This model is an ablation of removing the latent loss to see if the reconstruction loss can solely drive slots
# into learning representations that are useful for disentanglement.
# This model uses mechanisms to construct a cost matrix and do the matching to projected slots
# So there won't be any need for any other feature, like shape, color, CoM, etc. 
# However, as opposed to inertia_balls_saae_contrastive_recons, it only uses reconstruction loss and as opposed to
# inertia_balls_saae_contrastive_enc_only, this model does have a decoder, but it does NOT use any loss in the latent space.

_target_: src.models.inertia_balls_mechanism_match_recons_ablation_latent_loss.Inert

# put _self_ at the top so the overrides from this file take place (particularly for slot size)
defaults:
  - scheduler_config: null # options: reduce_on_plateau, linear, polynomial
  - optimizer: adamw # options: adamw, adam
  - encoder: slot_attention_autoencoder # options: resnet18_encoder, difference_model, slot_attention_autoencoder, slot_attention_encoder
  - additional_logger: reconstruction_logger


w_latent_loss: 0.0
w_recons_loss: 1.0
w_similarity_loss: 0.0

# model_name_or_path: ${work_dir}/models/

# set this to null to train from scratch.  NOTE: This path (.pt) only contains the 
# state_dict (to be flexible), if the full ckpt is required (not flexible w/ 
# different # of slots, then load .ckpt file)
encoder_ckpt_path: null
encoder_freeze: False

n_balls: ${datamodule.n_balls}
n_latents: ${mult_int:2,${model.n_balls}}
baseline: True
base_architecture: "resnet18"

contrastive:
    _target_: src.models.contrastive_pl.Contrastive
    base_architecture: ${model.base_architecture}
        
    alpha: 0.0
    baseline: ${model.baseline}
    normalize: True
    normalize_both: True
    p_norm: 2
    

# This model doesn't match to any target feature (like color, CoM, etc.), rather directly projects slots to target
# latent dimension and does the matching based on the cost matrix constructed by mechanism
# predictions per slot projection at time 't' and comparing them to slot projections at t+1
z_dim: 2

# Since the slot size is small, make sure to do a sweep over hid_dim

# ------------- no projection to latent space in this ablation -------------
# # set this to null to train from scratch
# projection_to_z_ckpt_path: null
# projection_to_z_freeze: False

# projection_to_z:
#   fc1:
#     _target_: torch.nn.Linear
#     in_features: ${model.encoder.slot_size} # 32
#     out_features: ${mult:${model.encoder.slot_size}, 0.5} # 32 * 0.5
#     bias: False

#   fc1_nonlinearity:
#     _target_: torch.nn.ReLU 

#   fc2:
#     _target_: torch.nn.Linear
#     in_features: ${model.projection_to_z.fc1.out_features} # 16
#     out_features: ${mult:${model.projection_to_z.fc1.out_features}, 0.5} # 16 * 0.5
#     bias: False

#   fc2_nonlinearity:
#     _target_: torch.nn.Tanh 
  
#   fc3:
#     _target_: torch.nn.Linear
#     in_features: ${model.projection_to_z.fc2.out_features}
#     out_features: ${model.z_dim} # 2
#     bias: False

  # fc3_nonlinearity:
  #   _target_: null


logging_name: "inertia_balls_with_decoder_ablation_latent_loss"
    