scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: 'min'
    factor: 0.8
    patience: 20
    threshold: 0.00001
    threshold_mode: 'abs'
    cooldown: 10
    min_lr: 1e-7
    eps: 1e-8
    verbose: True
    
scheduler_dict:
#     scheduler:  # scheduler instance, will be passed inside configure_optimizer
    interval: "step"  # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 50  # corresponds to updating the learning rate after every `frequency` epoch/step
    monitor: train_loss # Used by a LearningRateMonitor callback when ReduceLROnPlateau is used
    name: "ReduceLROnPlateau"